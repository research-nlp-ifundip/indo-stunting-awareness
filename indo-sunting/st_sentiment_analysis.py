# -*- coding: utf-8 -*-
"""ST_Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18h918vdIZUpS0qGnOeGEErJD1z8OTQ65
"""

import ast
import csv
import matplotlib.pyplot as plt
import nltk
nltk.download('punkt')
nltk.download('vader_lexicon')
import numpy as np
import pandas as pd
import re
import seaborn as sns
import sys
import warnings
warnings.filterwarnings("ignore")
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from pickle import dump
from scipy.sparse import hstack
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC

"""# Labelling with Vader (All Data)"""

df = pd.read_csv('dominant_topic.csv', names=["date", "content", "probability", "topics", "keyword", "content_prepro", "label"])
df = df.loc[df['label'] == 2]

# convert you string of a list of to an actual list
df['prepro_joined'] = df['content_prepro'].apply(ast.literal_eval)

# Join the tweet back together
df['prepro_joined'] = df['prepro_joined'].apply(lambda x: ' '.join(x))

"""Vader"""

# Analiza sentimenata NLTK Vader

def sentiment_analyze(stemmer):
  score = SentimentIntensityAnalyzer().polarity_scores(stemmer)
  neg = score['neg']
  pos = score['pos']
  neu = score['neu']

  return SentimentIntensityAnalyzer().polarity_scores(stemmer)

df['polarity']= df['prepro_joined'].apply(sentiment_analyze)

df['compound'] = df['polarity'].apply(lambda d:d['compound'])

df['sentiment'] = df['compound'].apply(lambda score: 'positive' if score>=0.05 else 'negative' if score<=-0.05 else 'neutral')

df.to_csv("sentiment_vader.csv", index=False, header=False)

tdm_transformer = CountVectorizer().fit(df['prepro_joined'])
df_tdm = tdm_transformer.transform(df['prepro_joined'])

tfidf_transformer = TfidfTransformer().fit(df_tdm)
df_tfidf = tfidf_transformer.transform(df_tdm)

# create a pipeline to convert the data into tfidf form
pipeline = Pipeline([
    ('bow', CountVectorizer()),  # strings to token integer counts
    ('tfidf', TfidfTransformer())])

# Specify X and y
X = pipeline.fit_transform(df.prepro_joined)
y = df.sentiment

# Train test split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15,random_state=123)

model = MultinomialNB()
params = {'alpha': np.linspace(0.5, 1.5, 6), 'fit_prior': [True, False]}
cv = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)
model_cv = GridSearchCV(model, param_grid=params, scoring='accuracy', cv=cv)
result = model_cv.fit(X_train, y_train)

print('\n\n NAIVE BAYES')
print('Best Score = %s' % result.best_score_)
print('Std = %s' % result.cv_results_['std_test_score'][result.best_index_])
print('Best Hyperparameter = %s' % result.best_params_)

# Testing
bestModel = result.best_estimator_
print('Testing')
print("R2: {:.4f}".format(bestModel.score(X_test, y_test)))
print()
y_true, y_pred = y_test, result.predict(X_test)
print(confusion_matrix(y_true, y_pred))
print(classification_report(y_true, y_pred))
print()

"""# Manual Labelling"""

df = pd.read_csv('topic_training.csv', names=["date", "content", "probability", "topics", "keyword", "content_prepro", "label"])
df_test = pd.read_csv('topic_testing.csv', names=["date", "content", "probability", "topics", "keyword", "content_prepro", "label"])

"""### TF-IDF"""

tdm_transformer = CountVectorizer().fit(df['content_prepro'])
df_tdm = tdm_transformer.transform(df['content_prepro'])

tfidf_transformer = TfidfTransformer().fit(df_tdm)
df_tfidf = tfidf_transformer.transform(df_tdm)

# create a pipeline to convert the data into tfidf form
pipeline = Pipeline([
    ('bow', CountVectorizer()),  # strings to token integer counts
    ('tfidf', TfidfTransformer())])

# Specify X and y
extract = pipeline.fit(df.content_prepro)
X_train = extract.transform(df.content_prepro)
y_train = df.label
X_test = extract.transform(df_test.content_prepro)
y_test = df_test.label

"""### Build Model

NB
"""

model = MultinomialNB()
params = {'alpha': np.linspace(0.5, 1.5, 6), 'fit_prior': [True, False]}
cv = StratifiedKFold(n_splits=10, random_state=100, shuffle=True)
model_cv = GridSearchCV(model, param_grid=params, scoring='accuracy', cv=cv)
result = model_cv.fit(X_train, y_train)

print('\n\n NAIVE BAYES')
print('Best Score = %s' % result.best_score_)
print('Std = %s' % result.cv_results_['std_test_score'][result.best_index_])
print('Best Hyperparameter = %s' % result.best_params_)

# Testing
bestModel = result.best_estimator_
print('Testing')
print("R2: {:.4f}".format(bestModel.score(X_test, y_test)))
print()
y_true, y_pred = y_test, result.predict(X_test)
print(confusion_matrix(y_true, y_pred))
print(classification_report(y_true, y_pred))
print()

"""RF"""

params = {'n_estimators': [10, 50, 100],
          'max_features': ['auto', 'sqrt', 'log2'],
          'max_depth' : [6,7,8],
          'criterion' :['gini', 'entropy']}

model = RandomForestClassifier(random_state=100)
cv = StratifiedKFold(n_splits=10, random_state=100, shuffle=True)
model_cv = GridSearchCV(model, param_grid=params, scoring='accuracy', cv=cv)
result = model_cv.fit(X_train, y_train)

print('\n\n RANDOM FOREST')
print('Best Score = %s' % result.best_score_)
print('Std = %s' % result.cv_results_['std_test_score'][result.best_index_])
print('Best Hyperparameter = %s' % result.best_params_)

# Testing
bestModel = result.best_estimator_
print('Testing')
print("R2: {:.4f}".format(bestModel.score(X_test, y_test)))
print()
y_true, y_pred = y_test, result.predict(X_test)
print(confusion_matrix(y_true, y_pred))
print(classification_report(y_true, y_pred))
print()

"""LR"""

params = {'solver':['newton-cg', 'lbfgs', 'liblinear'],
          'penalty':['none', 'l1', 'l2', 'elasticnet'],
          'C': [0.1,1,10,100,1000],}
model = LogisticRegression()
cv = StratifiedKFold(n_splits=10, random_state=100, shuffle=True)
model_cv = GridSearchCV(model, param_grid=params, scoring='accuracy', cv=cv)
result = model_cv.fit(X_train, y_train)

print('\n\n LOGISTIC REGRESSION')
print('Best Score = %s' % result.best_score_)
print('Std = %s' % result.cv_results_['std_test_score'][result.best_index_])
print('Best Hyperparameter = %s' % result.best_params_)

# Testing
bestModel = result.best_estimator_
print('Testing')
print("R2: {:.4f}".format(bestModel.score(X_test, y_test)))
print()
y_true, y_pred = y_test, result.predict(X_test)
print(confusion_matrix(y_true, y_pred))
print(classification_report(y_true, y_pred))
print()

"""SVM"""

params = {'C': [0.1,1,10,100],
          'gamma':[0.01,0.1,1,10, 100],
          'degree':[0,1,2,3,4,5,6],
          'kernel':['linear','rbf','poly','sigmoid']}
model = SVC()
cv = StratifiedKFold(n_splits=10, random_state=100, shuffle=True)
model_cv = GridSearchCV(model, param_grid=params, scoring='accuracy', cv=cv)
result = model_cv.fit(X_train, y_train)

print('\n\n SVM')
print('Best Score = %s' % result.best_score_)
print('Std = %s' % result.cv_results_['std_test_score'][result.best_index_])
print('Best Hyperparameter = %s' % result.best_params_)

# Testing
bestModel = result.best_estimator_
print('Testing')
print("R2: {:.4f}".format(bestModel.score(X_test, y_test)))
print()
y_true, y_pred = y_test, result.predict(X_test)
print(confusion_matrix(y_true, y_pred))
print(classification_report(y_true, y_pred))
print()

"""Save Model"""

final_model = LogisticRegression(C=100, penalty='l2', solver='liblinear')
final_model.fit(X_train,y_train)

# save model
dump(final_model, open('modelsentiment.pkl', 'wb'))
# save pipeline
dump(extract, open('modelpipesentiment.pkl', 'wb'))

"""Test Predict"""

from pickle import load

# Load Pipeline
pipeline = load(open('modelpipesentiment.pkl', 'rb'))

# Specify X
X = pipeline.transform(df_test.content_prepro)

# Load Model
final_model = load(open('modelsentiment.pkl', 'rb'))

# Predict
predictions = final_model.predict(X)

# Dataframe
y_predicts = pd.DataFrame(data = predictions, columns = ['sentiment'], index = df_test.index.copy())
df_out = pd.merge(df_test, y_predicts, how = 'left', left_index = True, right_index = True)
df_out.to_csv("test_predict.csv", index=False)

"""### Prediksi Data (Lay Disc)"""

df_predict = pd.read_csv('dominant_topic.csv', names=["date", "content", "probability", "topics", "keyword", "content_prepro", "label"])
df_predict = df_predict.loc[df_predict['label'] == 2]

from pickle import load

# Load Pipeline
pipeline = load(open('modelpipesentiment.pkl', 'rb'))

# Specify X
X = pipeline.transform(df_predict.content_prepro)

# Load Model
final_model = load(open('modelsentiment.pkl', 'rb'))

# Predict
predictions = final_model.predict(X)

# Dataframe
y_predicts = pd.DataFrame(data = predictions, columns = ['sentiment'], index = df_predict.index.copy())
df_out = pd.merge(df_predict, y_predicts, how = 'left', left_index = True, right_index = True)
df_out.to_csv("sentiment_predict.csv", index=False)

"""### The number of tweets by month and by laypeople’s sentiment"""

types_counts = df_out.groupby([df_out['date'].str[:7], df_out['sentiment']])['content'].count().reset_index(name="count")

# setting the dimensions of the plot
fig, ax = plt.subplots(figsize=(15, 8))
sns.lineplot('date', 'count', ci=None, hue='sentiment', data=types_counts, ax=ax, palette="tab10", legend=False, linewidth = 2)
plt.legend(loc='upper right', labels=["Negative", "Neutral", "Positive"])
plt.xlabel('')
plt.ylabel('Number of Tweets')
plt.savefig('tweet_sentiment_trend.png')
plt.show()

"""###Laypeople’s overall sentiment distribution on Stunting and their sentiment distributions across topics."""

topics_counts = df_out.groupby([df_out['topics'], df_out['sentiment']])['content'].count().reset_index(name="count")

pos = []
neg = []
net = []

for topic in range(0,4):
  total = topics_counts.loc[topics_counts['topics'] == topic, 'count'].sum()
  data = topics_counts.loc[topics_counts['topics'] == topic]

  positive = round((data.loc[topics_counts['sentiment'] == 1, 'count'].values[0])*100 / total, 2)
  pos.append(str(data.loc[topics_counts['sentiment'] == 1, 'count'].values[0])+" ("+ str(positive) +")")

  negative = round((data.loc[topics_counts['sentiment'] == -1, 'count'].values[0])*100 / total, 2)
  neg.append(str(data.loc[topics_counts['sentiment'] == -1, 'count'].values[0])+" ("+ str(negative) +")")

  neutral = round((data.loc[topics_counts['sentiment'] == 0, 'count'].values[0])*100 / total, 2)
  net.append(str(data.loc[topics_counts['sentiment'] == 0, 'count'].values[0])+" ("+ str(neutral) +")")

topic = ["Topic 1", "Topic 2", "Topic 3", "Topic 4"]
results = pd.DataFrame(list(zip(topic, pos, neg, net)),
               columns =['Topic', 'Positive (%)', 'Negative (%)', 'Neutral (%)'])
results