# -*- coding: utf-8 -*-
"""ST_Topic_Modelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DINFPt9SKUGnDPBml081_uwYocVBLbT9
"""

# Commented out IPython magic to ensure Python compatibility.
# !pip install -U pyLDAvis==2.1.2
# %%time
# import ast
# import csv
# import gensim
# import gensim.corpora as corpora
# import matplotlib.pyplot as plt
# import numpy as np
# import pandas as pd
# import pyLDAvis.gensim
# pyLDAvis.enable_notebook()
# import re
# import seaborn as sns
# import sys
# import tqdm
# import warnings
# warnings.filterwarnings("ignore")
# from gensim.models import CoherenceModel
# from gensim.utils import simple_preprocess
# from pprint import pprint
# from wordcloud import WordCloud, ImageColorGenerator

df = pd.read_csv('dataset_prediction2.csv', names=["date", "content", "content_prepro", "user", "preds"])

"""## Number of Tweets by Month"""

data_dist = df.groupby(df['date'].str[:7])['content'].count().reset_index(name="count")

plt.figure(figsize=(15, 8))
plt.bar(data_dist["date"], data_dist["count"])
plt.ylabel('Number of Tweets')
plt.show()

"""## Exploratory Analysis with WordCloud"""

# convert you string of a list of to an actual list
df['prepro_joined'] = df['content_prepro'].apply(ast.literal_eval)

# Join the tweet back together
df['prepro_joined'] = df['prepro_joined'].apply(lambda x: ' '.join(x))

# Join the different processed abstracts together.
long_string = ','.join(df['prepro_joined'].values)

# Generate the word cloud
wordcloud = WordCloud(width=1000, height=500,
                      background_color="white",
                      max_words=200,
                      contour_width=3,
                      contour_color='steelblue',
                      collocations=False).generate(long_string)

# Visualize the word cloud
# wordcloud.to_image()
wordcloud.to_file("wordcloud0.png") # Specify the path and filename

## Create stopword list:
stopwords = ['tp', 'gue', 'gitu', 'kek', 'apa', 'pas', 'lalu', 'dah', 'kapan', 'emang',
            'sai', 'lu', 'ada', 'gw', 'tak', 'padahal', 'gua', 'kamu', 'user', 'url',
            'emoji', 'hashtag', 'cuma', 'bukan', 'jg', 'se', 'kalau', 'ku', 'aku',
            'mu', 'malah', 'gin', 'nah', 'halo', 'deh', 'nder', 'mah', 'kayak', 'trus']

# Generate the word cloud
wordcloud2 = WordCloud(stopwords=stopwords, width=1000, height=500,
                      background_color="white",
                      max_words=200,
                      contour_width=3,
                      contour_color='steelblue',
                      collocations=False).generate(long_string)

# Visualize the word cloud
# wordcloud2.to_image()
wordcloud.to_file("wordcloud1.png") # Specify the path and filename

"""## Build N-gram"""

data_words = df["content_prepro"].values

# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count = 5, threshold=100)                                                # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)
# Faster way to get a sentence formatted as a bigram or trigram
bigram_mod = gensim.models.phrases.Phraser (bigram)
trigram_mod = gensim.models.phrases.Phraser (trigram)

"""Stopword"""

stop_words = ['tp', 'gue', 'gitu', 'kek', 'apa', 'pas', 'lalu', 'dah', 'kapan', 'emang',
            'sai', 'lu', 'ada', 'gw', 'tak', 'padahal', 'gua', 'kamu', 'user', 'url',
            'emoji', 'hashtag', 'cuma', 'bukan', 'jg', 'se', 'kalau', 'ku', 'aku',
            'mu', 'malah', 'gin', 'nah', 'halo', 'deh', 'nder', 'mah', 'kayak', 'trus']

# Define functions for stopwords
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]
def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)
# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

"""## Build Corpora"""

# Create Dictionary
id2word = corpora.Dictionary(data_words_bigrams)
# Create Corpus
texts = data_words_bigrams
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

"""## Build Topic Model"""

# Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                            id2word=id2word,
                                            num_topics=10,
                                            random_state=123, #for reproductibility
                                            chunksize=100,
                                            passes=10,
                                            alpha=0.01,
                                            eta='auto',
                                            iterations=400,
                                            per_word_topics=True)

# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. Lower value is preferred.

# Compute Coherence Score : degree of semantic similarity
coherence_model_lda = CoherenceModel (model=lda_model, texts=data_words_bigrams, dictionary= id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Coherence Score using UMass
coherence_model_lda = CoherenceModel (model=lda_model, texts=data_words_bigrams, dictionary= id2word, coherence='u_mass')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score u_mass: ', coherence_lda)

"""## Visualize Model"""

vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=id2word)
pyLDAvis.save_html(vis, 'lda_visualization.html')

"""##Find the Optimal number of Topics (Versi 1)"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    """
    Compute c_v coherence for various number of topics

    Parameters:
    ----------
    dictionary : Gensim dictionary
    corpus : Gensim corpus
    texts : List of input texts
    limit : Max num of topics

    Returns:
    -------
    model_list : List of LDA topic models
    coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    """
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                                num_topics=num_topics,
                                                id2word=id2word,
                                                random_state=123, #for reproductibility
                                                chunksize=100,
                                                passes=10,
                                                alpha=0.01,
                                                eta='auto',
                                                iterations=400,
                                                per_word_topics=True)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=id2word, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values

model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words_bigrams, start=2, limit=21, step=2)

# Show graph
limit=21; start=2; step=2;
x = range(start, limit, step)
plt.figure(figsize=(15,8))
plt.plot(x, coherence_values)
plt.title('Coherence Score for different number of topics', fontsize=14, fontweight="bold")
plt.xlabel("Number of Topics", fontsize=12)
plt.ylabel("Coherence score", fontsize=12)
plt.tick_params(axis='both', labelsize=13)
plt.legend(["Coherence Values"], loc='lower right')
plt.savefig('coherence_graph.png')
plt.show()

# Print the coherence scores for different number of topics
print('coherence score for different number of topics')
for m, cv in zip(x, coherence_values):
    print("Num Topics =", m, " has Coherence Value of", round(cv, 4))

"""Save List"""

np.save('model_list.npy', model_list)

np.save('coherence_values.npy', coherence_values)

model_list = np.load('model_list.npy', allow_pickle=True)

coherence_values = np.load('coherence_values.npy', allow_pickle=True)

"""## Hyperparameter Tuning (Versi 2)
https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0

Now that we have the baseline coherence score for the default LDA model, let’s perform a series of sensitivity tests to help determine the following model hyperparameters:
1. Number of Topics (K)
2. Dirichlet hyperparameter alpha: Document-Topic Density
3. Dirichlet hyperparameter beta: Word-Topic Density
"""

# supporting function
def compute_coherence_values(corpus, dictionary, k, a, b):

    lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=dictionary,
                                           num_topics=k,
                                           random_state=100,
                                           chunksize=100,
                                           passes=10,
                                           alpha=a,
                                           eta=b)

    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_bigrams, dictionary=id2word, coherence='c_v')

    return coherence_model_lda.get_coherence()

"""####(75% dan 100%)"""

print('\n\nHyperparameter Tuning 75%')
grid = {}
grid['Validation_Set'] = {}
# Topics range
min_topics = 2
max_topics = 11
step_size = 1
topics_range = range(min_topics, max_topics, step_size)
# Alpha parameter
alpha = list(np.arange(0.01, 1, 0.3))
alpha.append('symmetric')
alpha.append('asymmetric')
# Beta parameter
beta = list(np.arange(0.01, 1, 0.3))
beta.append('symmetric')
# Validation sets
num_of_docs = len(corpus)
corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25),
               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5),
               gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)),
               corpus]
corpus_title = ['75% Corpus', '100% Corpus']
model_results = {'Validation_Set': [],
                 'Topics': [],
                 'Alpha': [],
                 'Beta': [],
                 'Coherence': []
                }
# Can take a long time to run
if 1 == 1:
    pbar = tqdm.tqdm(total=540)

    # iterate through validation corpuses
    for i in range(len(corpus_sets)):
        # iterate through number of topics
        for k in topics_range:
            # iterate through alpha values
            for a in alpha:
                # iterare through beta values
                for b in beta:
                    # get the coherence score for the given parameters
                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, k=k, a=a, b=b)
                    # Save the model results
                    model_results['Validation_Set'].append(corpus_title[i])
                    model_results['Topics'].append(k)
                    model_results['Alpha'].append(a)
                    model_results['Beta'].append(b)
                    model_results['Coherence'].append(cv)

                    pbar.update(1)
    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)
    pbar.close()

"""####(100%)"""

print('\n\nHyperparameter Tuning 100%')
grid = {}
grid['Validation_Set'] = {}
# Topics range
min_topics = 2
max_topics = 11
step_size = 1
topics_range = range(min_topics, max_topics, step_size)
# Alpha parameter
alpha = [0.1, 0.01, 0.001]
alpha.append('symmetric')
alpha.append('asymmetric')
# Beta parameter
beta = [0.1, 0.01, 0.001]
beta.append('symmetric')

model_results = {
                 'Topics': [],
                 'Alpha': [],
                 'Beta': [],
                 'Coherence': []
                }
# Can take a long time to run
if 1 == 1:
    pbar = tqdm.tqdm(total=180)
    # iterate through number of topics
    for k in topics_range:
        # iterate through alpha values
        for a in alpha:
        # iterare through beta values
            for b in beta:
                # get the coherence score for the given parameters
                cv = compute_coherence_values(corpus=corpus, dictionary=id2word, k=k, a=a, b=b)
                # Save the model results
                model_results['Topics'].append(k)
                model_results['Alpha'].append(a)
                model_results['Beta'].append(b)
                model_results['Coherence'].append(cv)
                pbar.update(1)
    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)
    pbar.close()

tuning = pd.read_csv('lda_tuning_results.csv')

# Max Values
print('max values of coherence')
tuning.loc[tuning['Coherence'].idxmax()]

# fixed alpha = 0.01 and beta = symmetric
tuning_counts = tuning.loc[(tuning['Alpha'] == str(0.01)) & (tuning['Beta'] == 'symmetric')]
tuning_counts

# setting the dimensions of the plot
fig, ax = plt.subplots(figsize=(15, 8))
sns.lineplot('Topics', 'Coherence', ci=None, data=tuning_counts, ax=ax, palette="tab10", linewidth = 2)
plt.xlabel('Number of Topics')
plt.ylabel('Coherence')
plt.savefig('coherence_graph_best.png') # Specify the path and filename
plt.show()

"""## Optimal Model"""

lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=4,
                                           random_state=100,
                                           chunksize=100,
                                           passes=10,
                                           alpha=0.01,
                                           eta='symmetric')

# Select the model and print the topics
optimal_model = lda_model
model_topics = optimal_model.show_topics(formatted=False)
pprint(optimal_model.print_topics(num_words=10))

# Compute Perplexity
print('\nPerplexity: ', optimal_model.log_perplexity(corpus))  # a measure of how good the model is. Lower value is preferred.

# Compute Coherence Score : degree of semantic similarity
coherence_model_lda = CoherenceModel (model=optimal_model, texts=data_words_bigrams, dictionary= id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Compute Coherence Score using UMass
coherence_model_lda = CoherenceModel (model=optimal_model, texts=data_words_bigrams, dictionary= id2word, coherence='u_mass')
coherence_lda = coherence_model_lda.get_coherence()
print('Optimal Model')
print('\nCoherence Score u_mass: ', coherence_lda)

pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(optimal_model, corpus, dictionary=id2word)
pyLDAvis.save_html(vis, 'lda_visualization_final.html')

"""## Topic Probability"""

def format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data_words_bigrams):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each documen
    for i, row_list in enumerate(ldamodel[corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, probability and Keywords for each document
        top_prop =""
        for j, (topic_num, prop_topic) in enumerate(row):
            wp = ldamodel.show_topic(topic_num)
            topic_keywords = ", ".join([word for word, prop in wp])
            top_prop = top_prop+ ' '+str(topic_num)+' ('+str(round(prop_topic,4))+') '
            if j == 0:  # => dominant topic
              dominan = str(topic_num)

        sent_topics_df = sent_topics_df.append(pd.Series([top_prop, dominan, topic_keywords]), ignore_index=True)

    sent_topics_df.columns = ['Topic_Probability','Dominant_Topic', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)

df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data_words_bigrams)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Topic_Probability','Dominant_Topic', 'Topic_Keywords', 'Text_Prepro']
df_dominant_topic = df_dominant_topic.drop(["Document_No"], axis=1)
data = pd.read_csv('dataset_prediksi2.csv', names=["date", "content", "content_prepro", "user", "preds"])
df_dominant_topic = pd.concat([data["date"], data["content"], df_dominant_topic, data["preds"]], axis=1)
# df_dominant_topic.sort_values(['preds'], inplace=True) #sorting

df_dominant_topic.to_csv("dominant_topic.csv", index=False, header=False)

df_dominant_topic = pd.read_csv("dominant_topic.csv", names=['date', 'content','Topic_Probability','Dominant_Topic', 'Topic_Keywords', 'Text_Prepro', 'preds'])

"""##Word Clouds of Top N Keywords in Each Topic"""

# Generate the word cloud
cloud = WordCloud(width=1000, height=500,
                      background_color="white",
                      max_words=10,
                      contour_width=3,
                      contour_color='steelblue',
                      collocations=False)

topics = optimal_model.show_topics(formatted=False)

fig, axes = plt.subplots(1, 4, figsize=(30,30), sharex=True, sharey=True)

count = 1
for i, ax in enumerate(axes.flatten()):
    fig.add_subplot(ax)
    topic_words = dict(topics[i][1])
    cloud.generate_from_frequencies(topic_words)
    plt.gca().imshow(cloud)
    plt.gca().set_title('Topic ' + str(count) + '\n', fontdict=dict(size=40))
    plt.gca().axis('off')
    count +=1


plt.subplots_adjust(wspace=0, hspace=0)
plt.axis('off')
plt.margins(x=0, y=0)
plt.tight_layout()
plt.savefig('wordcloud_best.png')
plt.show()

"""## Number of Tweets by Topics"""

# Number of Documents for Each Topic
data_dist = df_dominant_topic.groupby(df_dominant_topic['Dominant_Topic'])['content'].count().reset_index(name="count").sort_values('count', ascending=False)

plt.figure(figsize=(15, 8))
plt.bar(data_dist["Dominant_Topic"].astype(str), data_dist["count"])
# for index,data in enumerate(data_dist["count"]):
#     plt.text(x=index , y =data+1 , s=f"{data}" , fontdict=dict(fontsize=12), ha='center')
plt.xticks((0, 1, 2, 3), ('Topic 3', 'Topic 2', 'Topic 4', 'Topic 1'))
plt.ylabel('Number of Tweets')
plt.savefig('number_of_tweet.png')
plt.show()

"""## The number of Stunting–related tweets by month and by tweet type (Promotional vs Lay)"""

type_counts = df_dominant_topic.groupby([df_dominant_topic['date'].str[:7], df_dominant_topic['preds']])['content'].count().reset_index(name="count")

# setting the dimensions of the plot
fig, ax = plt.subplots(figsize=(15, 8))
sns.lineplot('date', 'count', ci=None, hue='preds', data=type_counts, ax=ax, palette="tab10", legend=False, linewidth = 2)
plt.legend(loc='upper right', labels=['Information-PSA', 'Lay Discussion'])
plt.xlabel('')
plt.ylabel('Number of Tweets')
plt.savefig('number_of_tweet_by_month.png')
plt.show()

"""##Topic distribution of promotional by month and by topic"""

types_counts = df_dominant_topic.groupby([df_dominant_topic['date'].str[:7], df_dominant_topic['preds'], df_dominant_topic['Dominant_Topic']])['content'].count().reset_index(name="count")
promotional_counts = types_counts.loc[types_counts['preds'] == 1]

# setting the dimensions of the plot
fig, ax = plt.subplots(figsize=(15, 8))
sns.lineplot('date', 'count', ci=None, hue='Dominant_Topic', data=promotional_counts, ax=ax, palette="tab10", legend=False, linewidth = 2)
plt.legend(loc='upper right', labels=["Topic 1", "Topic 2", "Topic 3", "Topic 4"])
plt.xlabel('')
plt.ylabel('Number of Tweets')
plt.savefig('number_of_tweet_by_topic.png')
plt.show()

"""## Topic distribution of laydiscussions by month and by topic"""

lay_counts = types_counts.loc[types_counts['preds'] == 2]

# setting the dimensions of the plot
fig, ax = plt.subplots(figsize=(15, 8))
sns.lineplot('date', 'count', ci=None, hue='Dominant_Topic', data=lay_counts, ax=ax, palette="tab10", legend=False, linewidth = 2)
plt.legend(loc='upper right', labels=["Topic 1", "Topic 2", "Topic 3", "Topic 4"])
plt.xlabel('')
plt.ylabel('Number of Tweets')
plt.savefig('number_of_tweet_by_month_2.png')
plt.show()

"""####Pearson correlation coefficients between promotional and laypeople’s discussions based on their monthly tweet volumes"""

pearson_counts = df_dominant_topic.groupby([df_dominant_topic['date'].str[:7], df_dominant_topic['preds'], df_dominant_topic['Dominant_Topic']])['content'].count().reset_index(name="count")

import scipy.stats

correlations = []
pvalues = []

for topic in range(0,4):
  promotional = []
  lay = []
  for index in range(0,104):
    if pearson_counts['Dominant_Topic'][index] == str(topic):
      if pearson_counts['preds'][index] == 1:
        promotional.append(pearson_counts['count'][index])
      else:
        lay.append(pearson_counts['count'][index])
  coef, pval = scipy.stats.pearsonr(promotional, lay)
  correlations.append(coef)
  pvalues.append(pval)

print('Pearson correlation coefficients between promotional and laypeople’s discussions based on their monthly tweet volumes')
print(correlations)

topic = ["Topic 1", "Topic 2", "Topic 3", "Topic 4"]
pearson_results = pd.DataFrame(list(zip(topic, correlations, pvalues)),
               columns =['Topic', 'Correlation Coefficient', 'P-value'])
pearson_results